{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5j-sizuy448"
   },
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1550,
     "status": "ok",
     "timestamp": 1570241059057,
     "user": {
      "displayName": "Max Ruan",
      "photoUrl": "",
      "userId": "18331083944507779933"
     },
     "user_tz": -600
    },
    "id": "QKyULT0mF5Md",
    "outputId": "ea9a5b66-ca05-489e-aca3-9d3026493ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "import statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "import math\n",
    "from math import sqrt\n",
    "from math import isnan\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive', force_remount =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kw62UepmGLN6"
   },
   "source": [
    "### Week 3 - MA and ARMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tRUIFFSRGNgB"
   },
   "source": [
    "### 1. MA Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W3St4vXYGP2X"
   },
   "source": [
    "Model where you regress a variable against its current error and past error term. MA(1) model is:\n",
    "\n",
    "$y_t = \\phi_0 + u_t + \\theta_1 u_{t-1}$\n",
    "\n",
    "Again error term normally distributed with zero mean, constant variance and zero autocovariance.\n",
    "\n",
    "$u_t = N(0,\\sigma^2_u)$\n",
    "\n",
    "MA(q) model is:\n",
    "\n",
    "$y_t = \\phi_0 + u_t + \\theta_1 u_{t-1} + ... \\theta_q u_{t-q}$\n",
    "\n",
    "Note: p is lags for AR terms, q is lags for MA terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lB4H5iDmGW5e"
   },
   "source": [
    "### 2. Benefits of MA Models - Turning into its AR Form (Given Stationarity Assumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcCAy1p1GXqL"
   },
   "source": [
    "MA model can generate an infinite autogressive lag structure. Meaning if we wanted to generate a large-p AR(p) mode, we can do so even when the sample size is relatively small. It would be difficult to estimate the AR model but easy to estimate the MA model.\n",
    "\n",
    "Using lag operators, MA(1) model is:\n",
    "\n",
    "$y_t = \\phi_0 + u_t + \\theta_1 u_{t-1}$\n",
    "\n",
    "$y_t = \\phi_0 + (1+\\theta_1L) u_t$\n",
    "\n",
    "We multiply both sides by the inverse of $(1+\\theta_1L)$:\n",
    "\n",
    "$(1+\\theta_1L)^{-1} y_t = \\frac{\\phi_0}{(1+\\theta_1)} +  u_t$\n",
    "\n",
    "Question: why does the L do away?\n",
    "\n",
    "Given the assumption of stationarity ($|\\theta_1|<1$), this series follows a geometeric decaying progression that converges.\n",
    "\n",
    "$(1+\\theta_1L)^{-1} = 1 - \\theta_1 L + \\theta_1^2L^2 - ... + ...$\n",
    "\n",
    "This means the above result is:\n",
    "$(1 - \\theta_1 L + \\theta_1^2L^2 - ... + ...)y_t = \\frac{\\phi_0}{(1+\\theta_1)} +  u_t$\n",
    "\n",
    "Without lag operators:\n",
    "$y_t = \\frac{\\phi_0}{(1+\\theta_1)} + \\theta_1 y_{t-1} - \\theta^2_1 y_{t-2} + ... - ... + u_t$\n",
    "\n",
    "Here, given stationarity, a MA(1) model is the same as a restricted AR($\\infty$) model, where the AR parameters are restricted as:\n",
    "\n",
    "$\\phi_i = (-)^{i + 1} \\theta_1^i$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1CcIoisuLOuL"
   },
   "source": [
    "### 3. MA(1) Model Properties - Unconditional Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwET2wobMGjB"
   },
   "source": [
    "MA(1) model is:\n",
    "\n",
    "$y_t = \\phi_0 + u_t + \\theta_1 u_{t-1}$\n",
    "\n",
    "1. Take unconditional mean:\n",
    "\n",
    "$E(y_t) = \\phi_0 + E(u_t) + \\theta_1 E(u_{t-1})$\n",
    "\n",
    "2. Property of error term having zero mean:\n",
    "\n",
    "$\\mu = E(y_t) = \\phi_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5o4LdmdgMfuR"
   },
   "source": [
    "### 4. MA(1) Model Properties - Unconditional Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azBz1ik5Mg2h"
   },
   "source": [
    "MA(1) model is:\n",
    "\n",
    "$y_t = \\phi_0 + u_t + \\theta_1 u_{t-1}$\n",
    "\n",
    "1. Mean form:\n",
    "\n",
    "$\\mu = E(y_t) = \\phi_0$\n",
    "\n",
    "2. Subtract MA(1) model from mean form:\n",
    "\n",
    "$y_t - \\mu = u_t + \\theta_1 u_{t-1}$\n",
    "\n",
    "3. Square both sides:\n",
    "\n",
    "$(y_t - \\mu)^2 = u^2_t + \\theta^2_1 u^2_{t-1} + 2\\theta_1 u_t u_{t-1}$\n",
    "\n",
    "4. Take unconditional expectation:\n",
    "\n",
    "$\\gamma_0 = E((y_t - \\mu)^2) = E(u^2_t) + \\theta^2_1 E(u^2_{t-1}) + 2\\theta_1 E(u_t u_{t-1})$\n",
    "\n",
    "5. Remember $u_t$ is independent of $u_{t-1}$:\n",
    "\n",
    "$\\gamma_0 = E((y_t - \\mu)^2) = E(u^2_t) + \\theta^2_1 E(u^2_{t-1})$\n",
    "\n",
    "6. Apply homoskedastic error variance property:\n",
    "\n",
    "$\\sigma^2_u = E(u^2_t) = E(u^2_{t-1})$\n",
    "\n",
    "$\\gamma_0 = E((y_t - \\mu)^2) = \\sigma^2_u + \\theta^2_1 \\sigma^2_u = (1 + \\theta^2_1)\\sigma^2_u$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Br8At6wrODrh"
   },
   "source": [
    "### 5. MA(1) Model Properties - First Order Autocovariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "it0gmRdnOwIo"
   },
   "source": [
    "First Order Autocovariance defined as:\n",
    "\n",
    "$\\gamma_1 = E((y_t - \\mu)(y_{t-1} - \\mu))$\n",
    "\n",
    "1. Consider mean deviation form:\n",
    "\n",
    "$y_t - \\mu = u_t + \\theta_1 u_{t-1}$\n",
    "\n",
    "2. Substitute in equation:\n",
    "\n",
    "$\\gamma_1 = E((u_t + \\theta_1 u_{t-1})(u_{t-1} + \\theta_1 u_{t-2}))$\n",
    "\n",
    "3. Expand:\n",
    "\n",
    "$\\gamma_1 = E(u_tu_{t-1} + \\theta_1 u_tu_{t-2} + \\theta_1 u^2_{t-1} + \\theta^2_1u_{t-1}u_{t-2})$\n",
    "\n",
    "4. Apply fact that $u_t$ is independent to past lags:\n",
    "\n",
    "$\\gamma_1 = \\theta_1 E(u^2_{t-1}) = \\theta_1 \\sigma^2_u$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qGLnqPerQCFj"
   },
   "source": [
    "### 6. Second Order and $p$th Order Autocovariance (Check for MA(1) Model!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKu5X53LQHGD"
   },
   "source": [
    "Second Order Autocovariance defined as:\n",
    "\n",
    "$\\gamma_2 = E((y_t - \\mu)(y_{t-2} - \\mu))$\n",
    "\n",
    "1. Consider mean deviation form:\n",
    "\n",
    "$y_t - \\mu = u_t + \\theta_1 u_{t-1}$\n",
    "\n",
    "2. Substitute in equation:\n",
    "\n",
    "$\\gamma_2 = E((u_t + \\theta_1 u_{t-1})(u_{t-2} + \\theta_1 u_{t-3}))$\n",
    "\n",
    "3. Expand:\n",
    "\n",
    "$\\gamma_2 = E(u_tu_{t-2} + \\theta_1 u_tu_{t-3} + \\theta_1 u_{t-1}u_{t-2} + \\theta^2_1u_{t-1}u_{t-3})$\n",
    "\n",
    "4. Apply fact that $u_t$ is independent to past lags:\n",
    "\n",
    "$\\gamma_2 = 0$\n",
    "\n",
    "$p$th Order Autocovariance:\n",
    "\n",
    "$\\gamma_p = E((y_t - \\mu)(y_{t-p} - \\mu))$\n",
    "\n",
    "For the MA(1) model:\n",
    "\n",
    "$\\gamma_p = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L36U002QRO32"
   },
   "source": [
    "### 7. First Order and $p$th Order Autocorrelation for MA(1) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_G3GqEuRVhW"
   },
   "source": [
    "First Order Autocorrelation is:\n",
    "\n",
    "$\\rho_1 = \\frac{\\gamma_1}{\\gamma_0} = \\frac{\\theta_1 \\sigma^2_u}{(1 + \\theta^2_1)\\sigma^2_u} = \\frac{\\theta_1 }{(1 + \\theta^2_1)}$\n",
    "\n",
    "Second Order and $p$th Order Autocovariance is 0.\n",
    "\n",
    "This implies show that the ACF for a MA(1) has a spike at lag 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-M-TpXyScRp"
   },
   "source": [
    "### 8. MA Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27Wc-iPFSexx"
   },
   "source": [
    "Assume error term is distributed as $N(0,\\sigma^2_u)$, the MA parameters requires to be estimated are:\n",
    "\n",
    "$\\Theta = {\\phi_0, \\theta_1, \\theta_2, ..., \\theta_q, \\sigma^2_u}$\n",
    "\n",
    "We estimate these using maximum likelihood, due to the moving average component.\n",
    "\n",
    "We aim to choose parameters that maximise the following log likelihood function:\n",
    "\n",
    "$\\log L = -\\frac{1}{2}\\log 2\\pi - -\\frac{1}{2}\\log\\sigma^2_u - \\frac{1}{2\\sigma^2_uT} \\sum_{t=1}^{T} u^2_t$\n",
    "\n",
    "Where $u_t = y_t - \\phi_0 - \\theta_1u_{t-1} - \\theta_2u_{t-2} ...-\\theta_q u_{t-q}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLwPgwQvWO29"
   },
   "source": [
    "**Example: Unemployment Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 508,
     "status": "error",
     "timestamp": 1570240996216,
     "user": {
      "displayName": "Max Ruan",
      "photoUrl": "",
      "userId": "18331083944507779933"
     },
     "user_tz": -600
    },
    "id": "vJPZNMhAWfAs",
    "outputId": "fe61b227-f0ee-4fcf-ee5c-8f183225e388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-01b2d4dddbe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gdrive/My Drive/Colab Notebooks/Time Series/macro.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'gdrive/My Drive/Colab Notebooks/Time Series/macro.csv' does not exist: b'gdrive/My Drive/Colab Notebooks/Time Series/macro.csv'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "def parser(x):\n",
    "    return datetime.strptime(x, '%Y-%m-%d')\n",
    "series = pd.read_csv('gdrive/My Drive/Colab Notebooks/Time Series/macro.csv', parse_dates = [0], date_parser = parser)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1570171210260,
     "user": {
      "displayName": "Max Ruan",
      "photoUrl": "",
      "userId": "18331083944507779933"
     },
     "user_tz": -600
    },
    "id": "Lnk2BwOgXDwj",
    "outputId": "4608114e-e6e4-4c8c-fa2e-962bb0c278c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              ARMA Model Results                              \n",
      "==============================================================================\n",
      "Dep. Variable:                  URATE   No. Observations:                  199\n",
      "Model:                     ARMA(0, 1)   Log Likelihood                -306.881\n",
      "Method:                           mle   S.D. of innovations              1.116\n",
      "Date:                Fri, 04 Oct 2019   AIC                            619.762\n",
      "Time:                        06:40:08   BIC                            629.642\n",
      "Sample:                             0   HQIC                           623.761\n",
      "                                                                              \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const           6.2083      0.158     39.332      0.000       5.899       6.518\n",
      "ma.L1.URATE     1.0000      0.018     56.353      0.000       0.965       1.035\n",
      "                                    Roots                                    \n",
      "=============================================================================\n",
      "                  Real          Imaginary           Modulus         Frequency\n",
      "-----------------------------------------------------------------------------\n",
      "MA.1           -1.0000           +0.0000j            1.0000            0.5000\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Y = series['URATE']\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "model = ARMA(Y, order=(0, 1))\n",
    "model_fit = model.fit(disp=False,method='mle')\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 723,
     "status": "error",
     "timestamp": 1570171463030,
     "user": {
      "displayName": "Max Ruan",
      "photoUrl": "",
      "userId": "18331083944507779933"
     },
     "user_tz": -600
    },
    "id": "dhwMz6G8gayE",
    "outputId": "e8bb0ca1-0b50-4bb7-a4c1-7f6b2ec794c1"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-994684d2ad0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'URATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/statsmodels/tsa/arima_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, trend, method, transparams, solver, maxiter, full_output, disp, callback, start_ar_lags, **kwargs)\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# estimate starting parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m             start_params = self._fit_start_params((k_ar, k_ma, k), method,\n\u001b[0;32m--> 938\u001b[0;31m                                                   start_ar_lags)\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparams\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# transform initial parameters to ensure invertibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/statsmodels/tsa/arima_model.py\u001b[0m in \u001b[0;36m_fit_start_params\u001b[0;34m(self, order, method, start_ar_lags)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_start_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_ar_lags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'css-mle'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use Hannan-Rissanen to get start params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstart_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_start_params_hr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_ar_lags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use CSS to get start params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglike_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/statsmodels/tsa/arima_model.py\u001b[0m in \u001b[0;36m_fit_start_params_hr\u001b[0;34m(self, order, start_ar_lags)\u001b[0m\n\u001b[1;32m    538\u001b[0m         elif q and not np.all(np.abs(np.roots(np.r_[1, start_params[k + p:]]\n\u001b[1;32m    539\u001b[0m                                               )) < 1):\n\u001b[0;32m--> 540\u001b[0;31m             raise ValueError(\"The computed initial MA coefficients are not \"\n\u001b[0m\u001b[1;32m    541\u001b[0m                              \u001b[0;34m\"invertible\\nYou should induce invertibility, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                              \u001b[0;34m\"choose a different model order, or you can\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The computed initial MA coefficients are not invertible\nYou should induce invertibility, choose a different model order, or you can\npass your own start_params."
     ]
    }
   ],
   "source": [
    "Y = series['URATE']\n",
    "model = ARMA(Y, order=(0, 2))\n",
    "model_fit = model.fit(disp=False,method='mle')\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6jHxNSYhXgH"
   },
   "source": [
    "###9. MA Model Forecasting - Weakness with MA Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aoQ6jH9haSX"
   },
   "source": [
    "We want to know $y_{t+1}$.\n",
    "\n",
    "MA model tells us: $y_{t+1} = \\phi_0 + u_{t+1} + \\theta_1u_t$\n",
    "\n",
    "We have fitted values for $\\phi_0$ and $\\theta_1u_t$ and our expectation of $u_{t+1}$ is 0:\n",
    "\n",
    "$\\hat y_{t+1} = \\hat \\phi_0 + \\hat \\theta_1u_t$\n",
    "\n",
    "Note for 2-step ahead and further forecasts, we only get $\\hat \\phi_0$:\n",
    "\n",
    "$\\hat y_{t+2} = \\hat \\phi_0 + \\hat \\theta_1u_{t+1} = \\hat \\phi_0$\n",
    "\n",
    "This is simply a property of the MA(1) model.\n",
    "\n",
    "In general, for a MA(q) model, the forecasts for q + 1 periods ahead and onwards are all the same. This property commonly referred to having finite memory.\n",
    "\n",
    "In contrast, AR models have infinite memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-gkGdRLincw"
   },
   "source": [
    "### 10. ARMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSqYnMi_iqG_"
   },
   "source": [
    "ARMA(p,q) model can be used to allow for both AR and MA dynamics:\n",
    "\n",
    "$y_t = \\phi_0 + \\phi_1y_{t-1} + \\phi_2y_{t-2} + ... + \\phi_p y_{t-p} + u_t + \\theta_1 u_{t-1} + \\theta_2 u_{t-2} + ... + \\theta_q u_{t - q}$\n",
    "\n",
    "Written more compactly using lag operators:\n",
    "\n",
    "$y_t = \\phi_1 y_{t-1} - ... - \\phi_p y_{t-p} = \\phi_0 + u_t + \\theta_1 u_{t-1} + ... + \\theta_q u_{t-q}$\n",
    "\n",
    "$(1=\\phi_1 L - ... - \\phi_p L^p)y_t = (1 + \\theta_1 L + ... + \\theta_q L^q)u_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJMTbJFYozPF"
   },
   "source": [
    "### 11. Special Case ARMA(1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgdhzkbVswif"
   },
   "source": [
    "\n",
    "Remember MA(1) process can be shown to have infinite AR representation given stationarity. ARMA process can be the same, basically MA(1) given $\\phi_1 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2W35f8tXpMeA"
   },
   "source": [
    "### 12. ARMA Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOwl2HqlpPVc"
   },
   "source": [
    "Same as MA process estimation, use maximum likelihood estimation.\n",
    "\n",
    "We estimate the ARMA model perameters:\n",
    "\n",
    "Assume error term is distributed as $N(0,\\sigma^2_u)$, the MA parameters requires to be estimated are:\n",
    "\n",
    "$\\Theta = {\\phi_0, \\theta_1, \\theta_2, ..., \\theta_q, \\sigma^2_u, \\phi_1, \\phi_2, ..., \\phi_p}$\n",
    "\n",
    "We aim to choose parameters that maximise the following log likelihood function (same as MA L-L function):\n",
    "\n",
    "$\\log L = -\\frac{1}{2}\\log 2\\pi - -\\frac{1}{2}\\log\\sigma^2_u - \\frac{1}{2\\sigma^2_uT} \\sum_{t=1}^{T} u^2_t$\n",
    "\n",
    "Where $u_t = y_t - \\phi_0 - \\theta_1u_{t-1} - \\theta_2u_{t-2} ...-\\theta_q u_{t-q} - \\phi_1 y_{t-1} - ... - \\phi_p y_{t-p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 679,
     "status": "error",
     "timestamp": 1570240924971,
     "user": {
      "displayName": "Max Ruan",
      "photoUrl": "",
      "userId": "18331083944507779933"
     },
     "user_tz": -600
    },
    "id": "JDk59wIFpvUh",
    "outputId": "61f001b5-3fc2-4409-9c8b-70e88eb0de23"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-18446de8b1f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'URATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marima_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mARMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'series' is not defined"
     ]
    }
   ],
   "source": [
    "Y = series['URATE']\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "model = ARMA(Y, order=(1, 1))\n",
    "model_fit = model.fit(disp=False,method='mle')\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYZAvvQYqaFT"
   },
   "source": [
    "### 13. ARMA Model Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_cxt8_vqdwa"
   },
   "source": [
    "Same as before.\n",
    "\n",
    "ARMA models have long memory due to the AR component of the model.\n",
    "\n",
    "With ex-post forecasts, we can compare model performance by looking at the RMSE (lower is better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpC7u5vzqt-8"
   },
   "source": [
    "### 14. Pooling Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8sgV8eRrbxB"
   },
   "source": [
    "Instead of choosing between models and selecting the best one based on lowest RMSE.\n",
    "\n",
    "You can combine and pool the models.\n",
    "\n",
    "You have various model forecasts of $\\hat y_t$ and you  combine them by assigning a weight to each model $\\hat y_t$ and the weights add to 1.\n",
    "\n",
    "You can choose weights equally/unweighted, weighted by MSE, or by OLS regression.\n",
    "\n",
    "By pooling forecasts from an AR and MA model, this effectively generates forecasts for an ARMA model.\n",
    "\n",
    "Has no thereotical justification, only statistical performance benefits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnBqtmvDsEhd"
   },
   "source": [
    "### 15. ARMAX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIAvkipksHoO"
   },
   "source": [
    "Simply adds an explanatory variable to the ARMA model.\n",
    "\n",
    "Estimation down by nonlinear least squares.\n",
    "\n",
    "Forecasting requires some estimate or forecast or scenario analysis of your exogenous variables as well.\n",
    "\n",
    "Otherwise, just do a multivariate time series and model all variables together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbvWw19-slfU"
   },
   "source": [
    "### Week 4 - Multivariate VAR Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pqldu24Ms1uI"
   },
   "source": [
    "###1. VAR Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eb0UQ6HSYesg"
   },
   "source": [
    "We have N variables $[y_{1t},y_{2t}, ..., y_{Nt}]$\n",
    "\n",
    "Model is specified as:\n",
    "\n",
    "$y_t = \\Phi_0 + \\Phi_1 y_{t-1} + \\Phi_2 y_{t-2} + ... + \\Phi_p y_{t-p} + u_t$\n",
    "\n",
    "Basically we regress each variable by all variable lags. Hence, we have $N\\times 1$ constants $\\Phi_0$ and $N\\times p$ parameters and $N\\times 1$ error terms.\n",
    "\n",
    "The error terms are non-autocorrelated with zero mean and covariance, same as before - in vector notation, this is:\n",
    "\n",
    "\n",
    "$\\Omega = E(u_t u_t')$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILPZ0cuys5bh"
   },
   "source": [
    "###2. VAR Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAWyhsJeZibR"
   },
   "source": [
    "Estimation by doing $N$ number of OLS estimations for each $N$ variable, regressing on all variables for $p$ lags and a constant.\n",
    "\n",
    "OLS estimator for VAR properties:\n",
    "1.   Asymptotically efficient (i.e. produces smallest standard errors of any other estimator), because all explanatory variables are the same\n",
    "\n",
    "MLE estimator for VAR proeprties:\n",
    "1.   More useful if the set of explanatory variables in the VAR are not necessarily the same in each equation\n",
    "2.   More useful if VAR is extended to SVAR to allow for more structural information\n",
    "3. The df-adjusted log-likelihood is used to determine the overall lag length $p$ of the VAR\n",
    "\n",
    "MLE estimation involves:\n",
    "*   Again assuming $u_t$ is normally distributed with 0 mean and covariance matrix $\\Omega$\n",
    "*   Want to estimate parameters $\\theta = [\\Phi_0, \\Phi_1, ..., \\Phi_p, \\Omega]$\n",
    "*   Find parameters that maximise the conditional/average log-likelihood function: $\\frac{\\log L(\\theta)}{T-p}-\\frac{N}{2} - \\frac{1}{2}\\log |\\Omega|-\\frac{1}{2(T-p)}\\sum^T_{t=p+1}u_t'\\Omega^{-1}u_t$\n",
    "*   Hence, the MLE estimates for our parameters are $\\hat \\theta_{MLE} = \\arg \\min_\\theta \\frac{\\log  L(\\theta)}{T-p}$\n",
    "*   Equivalence Result: estimator equivalent to OLS estimator\n",
    "*   The MLE estimator of the error covariance matrix terms are $\\frac{1}{T}\\sum^T_{t=1}\\hat u_{it} \\hat u_{jt}$\n",
    "*   Log likelihood can be intepreted as the overall residual sum of squares for the system in a multivariate setting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKepIFw1s7gk"
   },
   "source": [
    "###3. VAR Lag Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTwL0e64cTtH"
   },
   "source": [
    "Use AIC, SC, HQ, measures that find the the highest R-squared or log-likelihood while penalising additional lags.\n",
    "\n",
    "$AIC = -2\\frac{\\log L(\\hat \\theta)}{T-p} + 2\\frac {K}{T-p}$\n",
    "\n",
    "$SC = -2\\frac{\\log L(\\hat \\theta)}{T-p} + 2\\frac {\\log (T-p) K}{T-p}$\n",
    "\n",
    "$HQ = -2\\frac{\\log L(\\hat \\theta)}{T-p} + 2\\frac {\\log(\\log(T-p))K}{T-p}$\n",
    "\n",
    "Where $K$ tells us the number of estimated parameters. Optimal lag structure minimises these statistics.\n",
    "\n",
    "Including redundant variables could give more information, but issues can result in inefficient parameter estimates (i.e. higher standard errors), less precise forecasts (i.e. higher RMSE).\n",
    "\n",
    "Information statistic properties:\n",
    "*   SC and HQ are consistent tests (choose correct lag structure as $T -> \\infty$)\n",
    "*   In small samples, SC tends to be too conservative and choose too short of a lag length\n",
    "*   Hence, HQ is generally the best choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Xty_1Yas-nV"
   },
   "source": [
    "###4. VAR Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8yQGnP0Gdy4v"
   },
   "source": [
    "One advantage of ex-post forecasting is it proivides an alternative lag-testing approach. i.e. use RMSE vs AIC, SIC, HQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryTrvjOCd82W"
   },
   "source": [
    "### Week 5 - VAR and Multivariate Causality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Pul9JPReC4I"
   },
   "source": [
    "### 1. Causality Analysis Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zb4NM2-CsAFa"
   },
   "source": [
    "VARs can contain many estimated parameters.\n",
    "\n",
    "We want to know which variables are important or not.\n",
    "\n",
    "You can do Granger causality tests, where you do joint tests on whetehr lags of other variabels help to predict $y$ having already included lags of $y$. i.e. Does including additional variables give better predictability?\n",
    "\n",
    "Granger causality tests test predictability/correlation, not causation.\n",
    "\n",
    "Note it is near impossible to detect causal relationships from just visually inspecting the series.\n",
    "\n",
    "In addition, a scatter plot can show the direction of association, but not identify the direction of causality.\n",
    "\n",
    "Same as correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3oYwpCt7tQ-z"
   },
   "source": [
    "### 2. Types of Granger Causality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "csnEup9tyS9U"
   },
   "source": [
    "1.   Biivariate causality (variable directly GC another variable)\n",
    "\n",
    "\n",
    "> a. Unidirectional fail to GC\n",
    "\n",
    "> b. Unidirectional causality\n",
    "\n",
    "> c. Bidirectional causality (or feedback)\n",
    "\n",
    "> d. Independence\n",
    "\n",
    "2.   Multivariate causality\n",
    "\n",
    "\n",
    "\n",
    "> a. X GC Y which then GC Z\n",
    "\n",
    "\n",
    "> b. X GC Y and Z but Y and Z are independent (drawn as a tree), note a bivariate test between Y and Z would mistakenly identify a causal link between the two (spurious result), also if Y and Z fail to cause X, then X is considered exogenous i.e. not affected by the system, variables that are not exogenous are endogenous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0w68EHQu5IK"
   },
   "source": [
    "### 3. Testing Granger Causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HmNcU2Xu8Br"
   },
   "source": [
    "Suppose you have a bivariate VAR with 2 lags.\n",
    "\n",
    "If you want to test if Y2 granger causes Y1, you test the null hypothesis:\n",
    "\n",
    "$H_0: \\phi_{13} = \\phi_{14}=0$ i.e. Y1 equation Y2 lags\n",
    "\n",
    "$H_1:$ at least one restriction fails\n",
    "\n",
    "We reject the null if the p-value is less than 0.05.\n",
    "\n",
    "\n",
    "In addition, which optimal alg structure, if optimal lag structure is 0, this implies no causal linkage between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnPSH7_jyUuE"
   },
   "source": [
    "### Week 6 - Multivariate Model Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpEiWDzsyc1G"
   },
   "source": [
    "### 1. Impulse Response Analysis Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcZDdt11nwA6"
   },
   "source": [
    "Granger causality tests relative importance of variables in a VAR.\n",
    "\n",
    "Granger causality doesn't tell us anything about the sign or direction of causality. It may only be obvious by looking at a 1 lag model and the sign of the coefficients. \n",
    "\n",
    "For more complicated models, this is more difficult. Hence, we compute impulse responses, which basically is feeding a 'shock' into the VAR and see how variables change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8l9X1w4up5Bv"
   },
   "source": [
    "### 2. Impulse Response Analysis - Cholesky Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVR7nRcHp-dW"
   },
   "source": [
    "With our regular VAR, we have have non-independent error terms from our equations. e.g. for a bivariate case, $E(u_{1t} u_{2t}) \\ne 0$. i.e. The error covariance matrix $\\Omega$ is non-diagonal.\n",
    "\n",
    "This is a problem because a shock in $u_{1t}$ is difficult to interpret as this is not independent of a shock in the second equation. You want to see see how a variable changes due a shock, we can't see the causal link as it could also be responsible to another shock.\n",
    "\n",
    "The aim is to obtain a strutural model where we have structural independent errors.\n",
    "\n",
    "We want to re-write the model (more specifically the way to compute parameter coefficients) where your second equation contains the $y_{1t}$ in the second equation for $y_{2t}$ as an explanatory variable. 1st equation is the same.\n",
    "\n",
    "The benefit is that we have new error terms $v_{1t}$ and $v_{2t}$ are independent i.e. $E(v_{1t} v_{2t}) = 0$.\n",
    "\n",
    "This transformation of a VAR is known as:\n",
    "1.   Cholesky decomposition\n",
    "2.   Recursive structural VAR (SVAR)\n",
    "\n",
    "e.g. If we had a trivariate case:\n",
    "*   1st equation remains the same\n",
    "*   2nd equation has the $y$ variable from the first equation\n",
    "*   3rd equation has the $y$ variables from the first and second equations\n",
    "\n",
    "Given this, the error terms are now independent. Off-diagonal elements of the new $\\Omega$ is 0. Zero covariances, this means LS is asympottically efficient.\n",
    "\n",
    "Given a shock in $v_{1t}$ of size $\\sigma_1$ in the first period.\n",
    "\n",
    "In the same first period:\n",
    "*   $y_{1t}$ changes by $\\sigma_1$\n",
    "*   $y_{2t}$ changes by $b_4\\sigma_1$\n",
    "*   $y_{3t}$ changes by $c_4\\sigma_1$\n",
    "\n",
    "Given a shock in $v_{2t}$ of size $\\sigma_2$ in the first period.\n",
    "\n",
    "In the same first period:\n",
    "*   $y_{1t}$ does not change as it has no $y_{2t}$ in the equation\n",
    "*   $y_{2t}$ changes by $\\sigma_2$\n",
    "*   $y_{3t}$ changes by $c_5\\sigma_2$\n",
    "\n",
    "Given a shock in $v_{3t}$ of size $\\sigma_3$ in the first period.\n",
    "\n",
    "In the same first period:\n",
    "*   $y_{1t}$ does not change as it has no $y_{3t}$ in the equation\n",
    "*   $y_{2t}$ does not change\n",
    "*   $y_{3t}$ changes by $\\sigma_3$\n",
    "\n",
    "To know what happens in the next period, rewrite the model for $y_{t+1}$ and see how our changes from period 1 affect it. You can also look at derivatives:\n",
    "$\\frac{\\partial y_{1t+1}}{\\partial v_{1t}} = a_1 \\frac{\\partial y_{1t}}{\\partial v_{1t}} = a_1 \\sigma_1$\n",
    "\n",
    "Know how to do this with the 3rd equation.\n",
    "\n",
    "We can estimate this system/SVAR by applyign least squares to each equation one at a time, given our new explanatory variables.\n",
    "\n",
    "The size of the shocks $\\Omega_1, \\Omega_2, \\Omega_3$ are the standard deviations of the error terms of each structural equation (standard errors of regression).\n",
    "\n",
    "The computed SE of regression may require an adjustment, you need to make a degrees of freedom adjustment. 1st equation stays the same. But the second equation now has an additional explanatory variable and thus has 1 less degree of freedom. Hence, you make the adjustment:\n",
    "$\\hat \\sigma_2 = 0.015900 \\times \\sqrt{(198 - 5)/(198 - 4))} = 0.015859$\n",
    "\n",
    "3rd equation now has 6 variables, not 5. Change 5 to 6.\n",
    "\n",
    "In Eviews, estimate the VAR, then click Impulse, 20 periods, no response standard errors, click OK. You can then plot the impulses. If you shock output for example, you can see the effect in different periods.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGj0HTQnyhKf"
   },
   "source": [
    "### 2. Variance Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2gGhSgdr0VXR"
   },
   "source": [
    "From Impulse Response, we can learn about the dynamics and the direction of causality, but not what is important or not.\n",
    "\n",
    "Variance decomposition allows us to do this, using the same information from the impulse response, figuring out the relative importance of shocks in the system.\n",
    "\n",
    "Impulse responses are about the conditional mean, variance decomposition is about the conditional variance.\n",
    "\n",
    "Consider the VAR. We obtain a SVAR, where the shocks $v_{1t}, v_{2t}, v_{3t}$ are independent.\n",
    "\n",
    "We define the 1-step forecast error variance of $y_{1t}$ conditional on information at $t - 1$ as:\n",
    "\n",
    "$e_{1t} = y_{1t} - E_{t-1}(y_{1t}) = v_{1t}$\n",
    "\n",
    "The one-step ahead forecast error for $y_{1t}$ is simply equal its own structural shock $v_{1t}$ because $E_{t-1}(v_{1t}) = 0$.\n",
    "\n",
    "The forecast error variance conditional on last periods information is:\n",
    "\n",
    "$var_{t-1}(e_{1t})=E_{t-1}(e^2_{1t}) = E_{t-1}(v^2_{1t}) = \\sigma^2_1$\n",
    "\n",
    "Because the forecast error mean is 0 and the definition of the error variance being $\\sigma^2_1$.\n",
    "\n",
    "Hence, for the first equation, we can decompose the forecast variance due to its own shocks ($y_{1t}$) or by other shocks ($y_{2t}, y_{3t}$).\n",
    "\n",
    "Total variance is: $var_{t-1}(e_{1t})$\n",
    "\n",
    "Variance due to its own shock: $\\sigma^2_1$\n",
    "\n",
    "Hence, the proportion due to itself is 1 or 100%, and the proportion due to other shocks is 0 or 0%.\n",
    "\n",
    "An estimate of the 1-step ahead forecast variance for $y_{1t}$ is: \n",
    "\n",
    "$\\hat var_{t-1}(e_{1t})=\\hat \\sigma^2_1 = 0.011799^2$\n",
    "\n",
    "Do the same for the forecast error variance of $y_{2t}$.\n",
    "\n",
    "Based on its expectation, the error variances $v_{1t}$ and $v_{2t}$ are 0. Hence, the difference is:\n",
    "\n",
    "$e_{2t} = y_{2t} - E_{t-1}(y_{2t}) = v_{2t} + b_4 v_{1t}$\n",
    "\n",
    "i.e. The forecast error is due to both variable 1 shocks and variable 2 shocks.\n",
    "\n",
    "The solve for the forecast error variance, we square the term $e_{2t} = y_{2t} - E_{t-1}(y_{2t}) = v_{2t} + b_4 v_{1t}$, expand it and obtain:\n",
    "\n",
    "$E_{t-1}(v^2_{2t} + b^2_4 v^2_{1t} + 2v_{2t}v_{1t})$\n",
    "\n",
    "Because we know that the error terms are independent and the definition of the error variances, the forecast error variance of $y_{2t}$ is:\n",
    "\n",
    "$var_{t-1}(e_{2t}) = \\sigma^2_2 + b^2_4 \\sigma^2_1$\n",
    "\n",
    "The proportion of forecast error variance 1-step ahead for $y_{2t}$ due to itself is $\\frac{b^2_4 \\sigma^2_1}{var_{t-1}(e_{2t})}$ and due to $y_{2t}$ shocks is $\\frac{\\sigma^2_2}{var_{t-1}(e_{2t})}$ and proportion due to $y_{3t}$ is 0.\n",
    "\n",
    "We know all these variables, so we can compute these variances and the proportion of the variance due to itself and others.\n",
    "\n",
    "Eview: VAR -> View / Variance Decomposition\n",
    "\n",
    "You can look at short-run (1 year) and long-run (5 year ) effects, where you can check the significant causal patterns between variables (e.g. based on contributions above 10%).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyHHcq49yjaC"
   },
   "source": [
    "###3. Diebold-Yilmaz Spillover Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAAmmIMoP72k"
   },
   "source": [
    "1 application of the variance decomposition.\n",
    "\n",
    "Used to show how markets are connected, their interdependence (where forecase error variance comes from, its own market or other markets) and performs network analysis.\n",
    "\n",
    "You pick a particular time horizon. e.g. if you are interested in a business cycle, maybe 5-year cycle.\n",
    "\n",
    "We obtain the variance decomposition matrix.\n",
    "\n",
    "The spillover index tells us the total variability that is due to spillovers between markets (i.e. not from itself). e.g. if you had 3 variables, and 90% of variability comes from itself, the idnex would be equal to 300% - 90%/300% = 210/300 = 70%. \n",
    "\n",
    "You build a VAR, based on the market returns, and choose the optimal lag based on AIC, SIC and HQ.\n",
    "\n",
    "Then compute the forecast error and variance based on the difference between the realised value and the expected value based on the VAR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXFTpSRNymwz"
   },
   "source": [
    "### 4. Nonrecursive SVARs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AOe4RFAWcqh"
   },
   "source": [
    "Sims critical of recursive (changing the ordering of the variables) macroeconomic models (variable 1 feeds into variable 2, variable 2 feeds into variable 3), because it is restrictibe in terms of economic theory.\n",
    "\n",
    "Cheolesky decomposition has the same criticism.\n",
    "\n",
    "The end of the day, ordering is not justified by economic theory.\n",
    "\n",
    "Hence, non-recursive structure seems to make more economic sense.\n",
    "\n",
    "Models can be estimated using Eviews, but outside of the course (estimation more involved).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHXAmUqYeqXR"
   },
   "source": [
    "### Week 7 - Nonstationary Models: Unit Roots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPDHeUPznx08"
   },
   "source": [
    "VAR variables are actually non-stationary.\n",
    "\n",
    "The question: can you estimate a model that is non-stationary? Depends on co-integration.\n",
    "\n",
    "We first test formally for non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ME6Vgx9Qe1nd"
   },
   "source": [
    "### 1. Random Walks vs. Trend Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "012FSSsmoKID"
   },
   "source": [
    "Two different models of non-stationary.\n",
    "\n",
    "We typically have variable non-stationarity of a random-walk type.\n",
    "\n",
    "We have previously looked at stationary in the mean or variance or higher moments such as skewness.\n",
    "\n",
    "\n",
    "2 broad methods for generating non-stationarity series:\n",
    "1.   Deterministic time trend (adds linear line going up $y_t = \\alpha + \\beta TREND_t + u_t$)\n",
    "2.   Random walk without drift ($y_t = y_{t-1} + u_t$) and with drift ($y_t = \\phi_0 + y_{t-1} + u_t$), stationary in the mean (without drift), non-stationary in the variance, non-stationary in the mean (with drift)\n",
    "\n",
    "E.g. With AR(1) model, if $\\phi_1 = 1$, then the model appears to be a random walk with drift. Models thus are nonstationary if this is very close to 1.\n",
    "\n",
    "We can generate models and get numbers close to 1. Is it due to standard error or is it statistically smaller than 1? We need a formal test.\n",
    "\n",
    "Nelson-Plosser said that almost all macro variables have the form of a random walk, rather than have a deterministic trend - this changes the way we analyse variables.\n",
    "\n",
    "What is the implication if we have a random walk variable?\n",
    "\n",
    "\n",
    "*   Forecasting: when you take conditional expectations and use OLS/MLE estimates for random walk with drift, we get $\\hat y_{t+h} = y_t + H\\hat \\phi_0$, forecast simply follows a linear time trend, without drift, you prediction is always $y_t$, which effectively tells you efficient market hypothesis - all public information is included in the current price \n",
    "\n",
    "Integradedness: How many times you first difference a variable to achieve stationarity\n",
    "*   I(1) non-stationary and random walk, differencing once to achieve stationarity\n",
    "*   I(0) already stationary\n",
    "*   I(2) differencing twice to achieve stationary, graphically more smoothly evolving, e.g. nominal economic series\n",
    "\n",
    "Random walk requires differencing once. \n",
    "\n",
    "There is no evidence that any economic series are I(3) or higher.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtGpRfXge5-J"
   },
   "source": [
    "### 2. Unit Root Tests and Testing Different Order Nonstationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iQXyE-AtIuC"
   },
   "source": [
    "We want to test for an AR(1) model if there is a unit root/nonstationarity.\n",
    "\n",
    "Null: = 1 i.e. I(1) / nonstationary\n",
    "Alternative: < 1 i.e. I(0) stationary\n",
    "\n",
    "Test by:\n",
    "*   Estimate AR(1) model\n",
    "*   Conduct t-test ($tstat = \\frac{\\hat \\phi_1 - 1}{se(\\hat \\phi_1)}$)\n",
    "*   However: this statistic does not follow a student-t distribution or normal distribution under the null hypothesis, the tables you choose depend on the null hypothesis, which is nonstationary, distribution of the D-F statistic is non-standard, the correct distribution is the D-F distribution, negatively skewed\n",
    "*   Look up the right p-value under the D-F distribution\n",
    "\n",
    "D-F Test:\n",
    "*   Start with AR(1) model\n",
    "*   Subtract $y_{t-1}$ from both sides, now the coefficient given unit root is 0\n",
    "*   This means we can now test for statistical significance and use the right p-value because Eviews automatically choosing the unit root test statistic and p-value\n",
    "*   Standard errors of the transformed coefficients are the same because we only did a linear transformation $se(\\hat \\phi_1) = se(\\hat \\beta)$\n",
    "*   Null is beta = 0 and beta being less than 0.\n",
    "*   Note: standard errors don't change, nor tstat value, p-value is incorrect (pick the right distribution for the t-statistic)\n",
    "*   In addition, D-F test assumes errors are white noise, but we fit a AR(1) model, without testing lags and fit, unlikely to meet this assumption\n",
    "\n",
    "Augmented D-F Test:\n",
    "*   Start with the VAR, find optimal p based on AIC, etc.\n",
    "*   Subtract $y_{t-1}$ from both sides\n",
    "*   You again test beta = 0 but you now have all the $p$ $y_{t-1} - y_{t-i-1}$ terms\n",
    "*   This aims to soak up autocorrelation from the residuals and get closer to white noise\n",
    "*   Essentially the difference is that we choose the number of lags for our model in which we test, and Eviews chooses this p based on the information criteria\n",
    "*   Null: Non-stationarity? Doe that imply I(1)? Could also be higher order non-stationarity\n",
    "*   You do that test again for the first-differenced series to see if its I(1) or I(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5A3nAdYyfAkc"
   },
   "source": [
    "### 3. Other Unit Root Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2RXz1Z4Z6zB"
   },
   "source": [
    "There are different unit root tests - the difference is the 'power' of a test - the probability of choosing the alternative when it is true:\n",
    "*   Extending to allow for time trend (resulting in different critical values)\n",
    "*   Allowing for structural breaks (resulting in different critical values)\n",
    "*   Phillips-Perron (nonparametrric autocorrelation adjustment)\n",
    "*   KPSS (null hypothesis is stationary)\n",
    "*   Elliott-Rothenberg-Stock (improvement in power)\n",
    "\n",
    "Interesting extension: price bubbles, where null is unit root, but alternative is phi > 1, indicatign explosive unit root or price bubble. Don't do whole sample, do rolling windows or expanding windows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gJvTegNfEWN"
   },
   "source": [
    "### Week 8 - Nonstationary Models: Cointegration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvI6Xg6ndSp5"
   },
   "source": [
    "Equilibrium talk and convergence doesn't seem to make sense if there is non-stationarity.\n",
    "\n",
    "It is possible, you don't talk about a static equillibrium (feed a shock, variable converges to a single point), we talk about dynamic long-run equilibrium (that changes all the time, but processes are attracted to each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KomgxEJ1fH0F"
   },
   "source": [
    "### 1. Long-Run Economic Behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjSx-YgRfLJu"
   },
   "source": [
    "### 2. Cointegration Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uINeucS0dp0a"
   },
   "source": [
    "We have 2 variables that must be non-stationary.\n",
    "\n",
    "Visually, you can do a scatter diagram of 2 variables and see a linear line.\n",
    "\n",
    "Consider you model a linear relationship between these 2 variables:\n",
    "\n",
    "$y_{1t} = \\beta_0 + \\beta_1 y_{2t} + e_t$\n",
    "\n",
    "If deviations do not persist for too long i.e. $e_t$ is I(0), then the movement of 1 variable is determined by the movement of another variable. This doesn't mean its white noise, it can exhibit autocorrelation.\n",
    "\n",
    "Cointegration is if the 2 variables are I(1), you model them in a linear relationship, and the difference/residual is stationary I(0), then the 2 variables are cointegrated.\n",
    "\n",
    "Key implication is that this equation is a long-run equation because the deviations are temporary, thus we move towards to this relationship.\n",
    "\n",
    "More generally, if 2 variables are I(2) and the errors in the linear relationship is lower, even I(1), it is cointegrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1TsVwvOnBYm"
   },
   "source": [
    "### 3. Spurious Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHK5Z2HKnEt2"
   },
   "source": [
    "Spurious correlation is where two variables don't have any economic connecting process, but they are found to be highly (positively or negatively) correlated.\n",
    "\n",
    "This can arise when 2 variables are non-stationary but are not cointegrated. \n",
    "\n",
    "You will find that the higher degree of non-stationarity, there tends to be higher correlation, generating high R-squareds when we model it, which is purely spurious!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAor3TdZfOje"
   },
   "source": [
    "### 3. Cointegration Equation Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cn4Ajos6pQfb"
   },
   "source": [
    "We start with a linear equation.\n",
    "\n",
    "We have different co-integrating estimators:\n",
    "*   Engle-Granger estimator i.e. LS (single equation)\n",
    "*   Johansen estimator (multiple equation)\n",
    "\n",
    "These estimators have an important statistical property:\n",
    "*   Because our variables are non-stationary and co-integrated,  we have super/T consistency where it converges to the population beta at rate T\n",
    "*   Our standard estimators, which assume stationarity, are consistent at the usual rate of root T\n",
    "\n",
    "Engle-Granger estimator i.e. LS (single equation)\n",
    "*   Regress using OLS\n",
    "*   Look at residuals to see if stationary, they may stil have autocorrelation, they are zero mean due to including a intercept\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZ4ERtmAfRnS"
   },
   "source": [
    "### 4. Cointegration Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_VxmKTirnRq"
   },
   "source": [
    "\n",
    "*   Estimate model using Engle-Granger / OLS.\n",
    "*   Then compute OLS residuals.\n",
    "*   Then apply ADF unit root test on residuals.\n",
    "\n",
    "However,the residuals is based on our estimated model, and we need to recognise the loss in the degrees of freedom used in the estimation of our model and thus our residuals, thus we need to generate the correct critical values and p-values.\n",
    "\n",
    "The Johansen cointegration test:\n",
    "*   Multivariate generalisation\n",
    "*   First stage: null no cointegration for N-variate system both variables only non-stationary, alternative: at least one cointegrating equation\n",
    "*   2nd stage: null one cointegratign equation, alternative at least two cointegrating equations\n",
    "*   Stage N: null N - 1 cointegrating equations, alternative: all variables stationary I(0)\n",
    "\n",
    "When doing this test, it gives us a eigenvalue, a trace statistic, the 0.05 critical value and the p-value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LYvsgqo502v"
   },
   "source": [
    "### Examples of Well Known Cointegrating Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZJQ2EoU54cZ"
   },
   "source": [
    "\n",
    "\n",
    "*   Term structure of interest rates\n",
    "*   Permanent income hypothesis (real consumption, real GDP)\n",
    "*   Money demand (real GDP, real money, interest rates)\n",
    "*   Fisher equation (interest rate, inflation)\n",
    "*   Present value model (stock price, dividends)\n",
    "*   Purchasing power parity (exchange rate, domestic and foreign prices)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg9_eUPkfZBj"
   },
   "source": [
    "### Week 9 - Nonstationary Models: VECM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7QQGcV6fcrk"
   },
   "source": [
    "### 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AJl8LR36yHS"
   },
   "source": [
    "\n",
    "\n",
    "*   We did unit root tests to test if variables are I(1) or I(2)\n",
    "*   We did cointegration tests to see if there is long-run cointegrating relationships i.e. error is I(0), etc.\n",
    "*   Now we aim to estimate this cointegrating equation, called a Vector Error-Correction Model or VECM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3ApyZXAff0D"
   },
   "source": [
    "### 2. Vector Error-Correction Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GVfg9h7w0SIx"
   },
   "source": [
    "\n",
    "\n",
    "*   Given a shock in the error term, there are 3 possible dynamics paths: variable y1 adjusts, variable y2 adjusts, or both adjust such that the long-term relationship holds\n",
    "*   The size of the disequilibrium is thus the error term\n",
    "*   The error correction model tells us that the change of the variable / theri dynamic time paths are determined by the size of the equilibrium and a 'speed of adjustment' variable\n",
    "\n",
    "$e_t = y_{1t} - \\beta_0 + \\beta_1 y_{2t}$\n",
    "\n",
    "$\\Delta y_{1t+1} = \\alpha_1 + \\gamma_1 e_t + v_{1t+1}$\n",
    "\n",
    "$\\Delta y_{2t+1} = \\alpha_2 + \\gamma_2 e_t + v_{2t+1}$\n",
    "\n",
    "The gamma parameters represent the 'error-correction' parameters. For example if $\\beta_1 > 0$ (positively sloped long-run equation), if $\\gamma < 0$, then $y_{1t}$ adjusts downwards.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpyBieEPfi0A"
   },
   "source": [
    "### 3. Estimation: Single Equation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cITxOM541qwU"
   },
   "source": [
    "We need to estimate the following parameters:\n",
    "\n",
    "$\\theta = [\\beta_0,\\beta_1,\\alpha_1,\\alpha_2,\\gamma_1,\\gamma_2]$\n",
    "\n",
    "i.e. The long-run relationship, the speed of adjustment and the direction of the change.\n",
    "\n",
    "Single equation methods are based on OLS applied to each equation one at a time.\n",
    "*   Advantage: parameter estimates are consistent (super-consistent in the case of $\\beta_1$ i.e. at rate T, rest at rate square root T)\n",
    "*   Disadvantage: not asympotically efficent, because estimators not computed jointly\n",
    "\n",
    "Steps:\n",
    "1.   Regress $y_{1t}$ on $c, y_{2t}$ to obtain cointegrating parameters $\\beta_0, \\beta_1$ using OLS\n",
    "2.   Obtain time series of residuals $\\hat e_{t-1}$\n",
    "3.   Regress $\\Delta y_{1t}$ on $c, \\hat e_{t-1}$ to obtain first equation parameters $\\alpha_1, \\gamma_1$ using OLS\n",
    "3.   Regress $\\Delta y_{2t}$ on $c, \\hat e_{t-1}$ to obtain second equation parameters $\\alpha_2, \\gamma_2$ using OLS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhV_oJZ5flca"
   },
   "source": [
    "### 4. Estimation: Systems Equation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8r6WmIF6239A"
   },
   "source": [
    "To achieve asymptotic efficiency, all parameters are estimated jointly using a MLE known as the Johansen estimator.\n",
    "*   Can perform hypothesis tests on VECM parameters based on asymptotic normality\n",
    "\n",
    "Implementing the Johansen estimator:\n",
    "*   Specify error distribution of N-variance VECM, which we usually assume as multivariate normality, $v_t = (v_{1t},v_{2t},...,v_{Nt})' -> N(0,\\Omega)$\n",
    "*   $\\Omega = E(v_t v_t')$ is the covariance matrix of the VECM errors\n",
    "*   The average log-likelihood of the multivariate normal distribution is: $\\frac{\\log L(\\theta)}{T} = -\\frac{N}{2} - \\frac{1}{2}\\log |\\Omega|-\\frac{1}{2T}\\sum^T_{t=2} v_t' \\Omega^{-1} v_t$\n",
    "*   For a model, use the first VECM equation and substitute the error term from the long-run cointegrated equation, so we have only y variable terms, subtituting out errors\n",
    "*   We need to estimate the following parameters: $\\theta = [\\beta_0,\\beta_1,\\gamma_1,\\gamma_2, \\sigma^2_1, \\sigma^2_2, \\sigma_{12}]$\n",
    "*   Choose the unknown parameters to maximise the average log-likelihood\n",
    "\n",
    "In Eviews:\n",
    "*   Open VAR ... / Vector Error Correction\n",
    "*   Change lag structure to 0 lags , 0 0 \n",
    "*   We need more lag veriables to ensure VECM errors are white noise, which is required as assumption\n",
    "*   How we choose the lag structure, we obtain the restricted VAR, where if VAR lag structure is p, VECM lag structure is p - 1, hence we can determine this using information criteria for a lag length p for a VAR where the variables are expressed in levels provided the variables are cointegrated (if not, variables should be expressed in first differences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2c23tUyfoUo"
   },
   "source": [
    "### 5. Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8OgHhD8MpE-"
   },
   "source": [
    "### Week 10 - Volatility Models GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwNBLL1rMuum"
   },
   "source": [
    "### 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Y4OFTwKMxJB"
   },
   "source": [
    "Switch to the mean to the variance.\n",
    "\n",
    "Useful in risk management, VaR, option pricing, long-run marginal expected short-fall, predictive regressions, interaction between volatility and the real economy.\n",
    "\n",
    "Models of volatility:\n",
    "*   Generalised Autoregressive Conditional Heteroskedasticity (GARCH)\n",
    "*   Realised Volatility/Variance (RV)\n",
    "*   Stochastic Volatility (SV)\n",
    "*   Implicit Volatility (IV)\n",
    "*   Range Volatility\n",
    "\n",
    "We see with return series tends to have bursts of volatility with positive autocorrelation in the variance, with quiet and more volatile periods, almost like 2 regimes. You are now saying the variance is non-constant over time, hence heteroskedastic.\n",
    "\n",
    "Descriptive statistics are based under the assumption of a constant variance, which suggests misspecification.\n",
    "\n",
    "Autocorrelation of returns shows no significant AR relationship, based on the ACF and PACF, i.e. no predictability.\n",
    "\n",
    "However, we see autocorrelation in squared returns, i.e. predictability.\n",
    "Note: this is the variance of returns provided the mean return is zero. If it is ver small, we could treat squared returns as a reasonable approximation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOSoitm7AD2d"
   },
   "source": [
    "### 2. Testing for Constant or Time-Varying Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGKYzc5tAG8F"
   },
   "source": [
    "Obtain squared-demeaned return series i.e. variance.\n",
    "\n",
    "Do a AR equation and check the ACF/PACF.\n",
    "\n",
    "Test the following hypothesesis:\n",
    "*   Null all coefficients for AR terms are jointly 0 (constant variance)\n",
    "*   At least one restriction is violated (time-varying variance)\n",
    "\n",
    "This test represents a preliminary test for ARCH and denoted as ARCH(q).\n",
    "\n",
    "Returns are represented as:\n",
    "$r_t = \\phi_0 + u_t$\n",
    "\n",
    "Squared demeaned returns is the variance of the series:\n",
    "$u_t^2 = (r_t - \\phi_0)^2$\n",
    "\n",
    "In practice, we would estimate the above return model and extract the model residuals $\\hat u_t$.\n",
    "\n",
    "Then you estimate a ARCH(q) or a AR(q) model for the variance:\n",
    "\n",
    "$\\hat u_t^2 = \\delta_0 + \\delta_1 \\hat u_{t-1}^2 + ... + \\delta_q \\hat u_{t-q}^2 + v_t$\n",
    "\n",
    "Then you compute the test statistic: \n",
    "\n",
    "$ARCH = TR^2$ where T is the sample size and $R^2$ is the coefficient of determination from the regression.\n",
    "\n",
    "Under the null hypothesis of constant variance, the test statistic is distributed under a chi-squared distribution with q degrees of freedom. We can estract the appropriate critical values and p-values from this distribution.\n",
    "\n",
    "Given this, if the test statistic p-value is greater than 0.05, we fail to reject the null at the 5% level. If it is less than 0.05, we reject the null in favour of the alternative. This procedure is formally known as the Lagrange multiplier test.\n",
    "\n",
    "Eviews: Quick / Estimate Equation / r c / View / Residual Diagnostics / Heteroskedasticity Tests / ARCH / q / Ok.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQLvaeaADzsa"
   },
   "source": [
    "### 3. Autoregressive Conditional Heteroskedasticity (ARCH) Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lA4XxlFND4zo"
   },
   "source": [
    "ARCH models capture autocorrelation in the variance. \n",
    "\n",
    "The variance is determined by the size of previous shocks.\n",
    "\n",
    "e.g. the ARCH(1) model is:\n",
    "\n",
    "$r_t = \\phi_0 + u_t$\n",
    "\n",
    "$h_t = \\alpha_0 + \\alpha_1 u_{t-1}^2$\n",
    "\n",
    "$u_t -> N(0,h_t)$\n",
    "\n",
    "THe key features is that the conditional mean is a constant, and the distirbution of $u_t$ is normally distributed.\n",
    "\n",
    "$f(r_t|r_{t-1},r_{t-2},..,\\theta) = \\frac{1}{\\sqrt{2\\pi h_t}} \\exp(-\\frac{(r_t - \\phi_0)^2}{2 h_t})$\n",
    "\n",
    "Interpretation is that small shocks $u_{t-1}$ will result in a small variance $h_t$ and vice-versa. Large shocks will result in an increase in variance in the next period if $\\alpha_1 > 0$.\n",
    "\n",
    "The \"News Impact Curve\" is a plot of $h_t$ against $u_t$. We see shocks of the same magnitude have the same effect on $h_t$ (no asymmetry effects).\n",
    "\n",
    "A very specifal case of the ARCH(1) model is a constant variance $\\alpha_1 = 0$. \n",
    "\n",
    "The model can be generalised to have more lags.\n",
    "\n",
    "Estimating the ARCH model parameters:\n",
    "\n",
    "$\\theta = [\\phi_0, \\alpha_0, \\alpha_1, .., \\alpha_q]$\n",
    "\n",
    "We estimate using maximum likelihood using an iterative alogorithm. Given conditional normality, the log-likelihood function is:\n",
    "\n",
    "$\\log L(\\theta) = \\frac{1}{T} \\sum^T_{t=1} \\log (r_t|r_{t-1},r_{t-2},...,\\theta)$\n",
    "\n",
    "$-\\frac{1}{2} \\log (2\\pi) - \\frac{1}{2T}\\sum^T_{t=1} \\log h_t - \\frac{1}{2T} \\sum^T_{t=1} \\frac{u^2_t}{h_t}$\n",
    "\n",
    "Where $u_t = y_t - \\phi_0$\n",
    "\n",
    "$h_t = \\alpha_0 + \\sum^q_{i = 1} \\alpha_i u^2_{t-1}$\n",
    "\n",
    "Eviews: Quick / Estimate Equation / r c / Estimation settings: ARCH / ARCH order 1 / GARCH order 0 / OK\n",
    "\n",
    "View / GARCH Graph / Conditional Variance/ Proc / Make GARCH Variance Series / OK\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZtc8vkaS6Ta"
   },
   "source": [
    "### 4. Is the Model a Good Fit? Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ei6XYQcaS-S-"
   },
   "source": [
    "We have estimated the ARCH model.\n",
    "\n",
    "Is the model any good?\n",
    "\n",
    "If the model is specified correctly, then the standardised reisiduals \n",
    "\n",
    "$z_t = \\frac{u_t}{\\sqrt{h_t}}$, the variance of it should be 1, i.e. $var(z_t) = 1$ and that there should be no evidence of autocorrelation in the variance. \n",
    "\n",
    "This is because our model assumptions is that the error term is normally distributed with 0 mean and variance $h_t$\n",
    "\n",
    "Also, we want to check if we have the true process. E.g. What if the true process is an ARCH(2) when we have an ARCH(1)?\n",
    "\n",
    "$Var(z_t) = \\frac{var(u_t)}{h_t}$\n",
    "\n",
    "If the correct model specification is done, that the true $var(u_t)$ and $var(h_t)$ are the same.\n",
    "\n",
    "However, if for example the $var(u_t)$ is ARCH(3) and $h_t$ is ARCH(1), then the ratio would be ARCH(2) as the variance expressions in the numerator and denominator would not cancel out.\n",
    "\n",
    "$u_t$ is the true variance and $h_t$ is the specified variance from the model.\n",
    "\n",
    "Hence, we want to test the following hypothesis:\n",
    "*   Null: model is correctly specified where $z_t$ variance is constant\n",
    "*   Alternative: model is misspecified and $z_t$ is time-varying\n",
    "\n",
    "i.e. We should see white-noise standardised residuals.\n",
    "\n",
    "Eviews: View / Residual Diagnostics / ARCH LM Test / Number of Lags: 1 (is there another lag in the true model?)\n",
    "\n",
    "We would get the ARCH test statistic and see its relevant p-value based on the chi-square distribution under trhe null. If less than 0.05, we reject the null of correct specification and conclude that the ARCH variance specification has higher order dynamics than ARCH(1) for example.\n",
    "\n",
    "We keep estimating a new model, reapplying the test until we can no longer reject the null.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o62QBTzBWh4Q"
   },
   "source": [
    "### 5. Generalised ARCH (GARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAYrV_xdWkeS"
   },
   "source": [
    "When estimating ARCH models, you find that you generally need a fairly long q. i.e. Volatility seems to have long-memory and is persistent. \n",
    "\n",
    "This can mean using a lot of degrees of freedom and lags.\n",
    "\n",
    "Like MA, we can capture this persistence where variance tends to cluster in periods while controlling for the number of parameters, we add an additional explanatory variable in the lagged conditional variance. \n",
    "\n",
    "So the GARCH term is responsible for capturing volatility persistence and memory features of volatiliy, which means that many lagged volatilities has predictability on current and future volatility. The GARCH terms allow for an infinite ARCH structure. Same ARMA for volatility.\n",
    "\n",
    "$h_t = \\alpha_0 + \\alpha_1 u^2_{t-1} + \\beta_1 h_{t-1}$\n",
    "\n",
    "We can show GARCH(1,1) to be a infinite ARCH model with just this additional parameter $\\beta_1$.\n",
    "\n",
    "We have the GARCH(1,1) model:\n",
    "$(1-\\beta_1 L)h_t = \\alpha_0 + \\alpha_1 u^2_{t-1}$\n",
    "\n",
    "Assuming $|\\beta_1|<1$, then we can invert the term:\n",
    "$h_t = (1-\\beta_1L)^{-1} \\alpha_0 + (1-\\beta_1L)^{-1} u^2_{t-1}$\n",
    "\n",
    "$= \\frac{\\alpha_0}{1-\\beta_1}+\\alpha_1(1+\\beta_1 L + \\beta_1^2 L^2 + ...)u^2_{t-1}$\n",
    "\n",
    "$= \\frac{\\alpha_0}{1-\\beta_1}+(\\alpha_1 u^2_{t-1} + \\alpha_1 \\beta_1 u^2_{t-2} + \\alpha_1 \\beta^2_1 u^2_{t-3} + ...)$\n",
    "\n",
    "Hence, we have an infinite order ARCH model.\n",
    "\n",
    "The $\\beta_1$ effectively tells us the effect of past shocks. The bigger the number, the bigger the strength of previous periods and thus, the longer the memory.\n",
    "\n",
    "We can allow for q lags of the ARCH term and p lags of the GARCH terms, resulting in GARCH(p,q) model.\n",
    "\n",
    "Again model assumptions have normal error term and conditional mean being constant.\n",
    "\n",
    "The GARCH(1,1) model generally is the best model. The estimates of $\\beta_1$ is around 0.9 and the $\\alpha_1$ term is around 0.05.\n",
    "\n",
    "Special case is ARCH(1) model is the constant variance model.\n",
    "\n",
    "Given the assumption of normality, we can estimate the GARCH model parameters:\n",
    "\n",
    "$\\theta = [\\phi_0, \\alpha_0, \\alpha_1, ..., \\alpha_q,\\beta_1,\\beta_2,...,\\beta_p]$\n",
    "\n",
    "Using MLE. The log-likelihood is:\n",
    "\n",
    "$\\log L(\\theta) = \\frac{1}{T}\\sum^T_{t=1} \\log f(r_t|r_{t-1},r_{t-2},...;\\theta)$\n",
    "\n",
    "$=1\\frac{1}{2}\\log 2\\pi - \\frac{1}{2T}\\sum^T_{t=1} \\log h_t - \\frac{1}{2T} \\sum^T_{t=1} \\frac{u^2_t}{h_t}$\n",
    "\n",
    "$u_t = r_t-\\phi_0$\n",
    "\n",
    "$h_t = \\alpha_0 + \\sum^q_{i=1}\\alpha_i u^2_{t-1} + \\sum^p_{i=1} \\beta_i h_{t-i}$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "H5j-sizuy448",
    "ME6Vgx9Qe1nd",
    "LtGpRfXge5-J",
    "5A3nAdYyfAkc",
    "9gJvTegNfEWN",
    "SjSx-YgRfLJu",
    "x1TsVwvOnBYm",
    "EAor3TdZfOje",
    "DZ4ERtmAfRnS",
    "3LYvsgqo502v",
    "Y3ApyZXAff0D",
    "GwNBLL1rMuum"
   ],
   "name": "TS Notes 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
